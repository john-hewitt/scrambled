## HW 6, Replicating “Financial Incentives and the Performance of Crowds”
By Alice Ren, John Hewitt, and Jie Luo

#### “Financial Incentives and the Performance of Crowds”
 
This paper discusses two experiments performed with Mechanical Turk workers that measured amount and quality of work done in relation to different pay grades and task difficulties. In the first experiment participants were asked to order traffic images from an image bank. Higher difficulties involved more images to be ordered per set. Participants were randomly given one of three pay grades and one of three difficulty levels, without indication that pay or difficulty would be any different for another person. Results found that across all difficulty levels participants chose to complete more tasks when payment was higher, but higher payment did not correlate with higher accuracy. 

The second experiment used word search puzzles as a task and compared the payment models of pay per word found in the puzzle and pay per puzzle solved. Results showed that there was no significant impact on quantity of work done between payment models. However, participants under the pay-per word model earned roughly 4 times as much per word, but ended up finding less words per puzzle than those paid per puzzle and unpaid. This research supports the claim that quota systems bring more effort than piece-rate systems.

#### “Crowds in Two Seconds: Enabling Realtime Crowd-Powered Interfaces”

In this paper, researchers experimented with different ways to lower crowd latency and create synchronous crowds to do complex tasks relatively quickly. The first technique they used, called the retainer model, works by paying workers to wait for tasks to arrive and responding quickly when they do. They found that this was more effective for shorter wait periods (such as 1 minute of wait time as opposed to 10), but that they could increase responsiveness even over longer periods of time by paying a small bonus for quick response times — providing a bonus over a 10-minute wait time was found to be just about as effective as having a 2-minute wait time with no bonus. The second technique they came up with, called rapid refinement, was used to efficiently aggregate results from synchronous crowds by continually narrowing the search space based on early signs of agreement between independent workers. This technique was shown to be faster than any individual crowd member alone, and the quality of the results, while not as good as results generated by other methods (“Generate-and-Vote”), was reasonable and had less variance than that of individual workers. The main test for these techniques was a camera application called Adrenaline, which uses crowds to find the best frame from a 10-second video to use as a photograph; the end results indicate that using these two methods, it is indeed possible to use on-demand synchronous crowds for intelligence tasks.


#### “Labeling Images with a Computer Game”

In this paper, researchers created and published a computer game called the ESP game that involved labeling images. In the game, two random players are presented an image. Both must type and submit the same word in order to score a point and progress to the next image. These images and words are usually a name of an object in the image or something that describes the image. Players can also skip images if they both agree. After an image has been labeled, the label word and derivations of the word become unusable words for the next pair that is presented the image. This ensures that images are labeled with multiple different words. Players can also be matched with bots that will submit answers from an answer bank.

After running the experiment for 4 months, the researchers tested the labeling quality by taking images with 6 labels and asking participants who had not played the game to label 20 of the images. For all of the images, at least 5 out of 6 labels produced by the game were entered by at least one participant. Participants were then given images and their generated labels and asked if any of the words had nothing to do with the image. They found that only 1.7% of the labels did not fit their respective image. In conclusion, the game is an effective way of gathering accurate labels that is more enjoyable and appealing than working a labeling task.

#### In-Depth Summary of “Financial Incentives and the Performance of Crowds”

Researchers conducted two experiments to investigate relationships between financial compensation and performance of tasks. The Mechanical Turk framework was chosen to be the experiment ground because it allowed limiting the work to the participant at his or her computer, eliminating many confounding factors that are apparent in real-life, such as group interaction and free-riding. 

In the first experiment, researchers hoped to differentiate the quantity and accuracy of the work in relation to cost incentives and difficulty of work. In the first image, workers were asked to sort traffic images taken at 2-second intervals in chronological order. This task was posted on Mechanical Turk with a base pay rate of $0.10. Upon accepting the hit, the participants were given an optional demographic survey and a training set. After the training set was completed, workers were given bonus sets and shown how much they would be paid for each bonus set they did. Workers were assigned a random difficulty — easy (2 images), medium (3), or hard (4) — and a randomly assigned pay rate — $0.00, $0.01, $0.05, or $0.10 per set. Participants could then sort up to 99 image sets or stop whenever they chose to.

The experiment had a total of 611 participants. The demographics survey showed a fairly diverse distribution across locations, genders, and income levels. Across all difficulty levels, participants completed more tasks when pay was higher. Researchers found that as pay increased, accuracy did not substantially improve and, in some cases, even decreased slightly. Data gathered from the questionnaire showed that the perceived value of the task was always higher than the actual paid value. The researchers attributed this lack of accuracy improvement to the anchoring effect; because workers in each pay grade felt they deserved more, they were no more motivated to perform exceptionally as pay increased.

![Paper quantity graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/Financial%20Incentives%20Tasks%20over%20Pay%20graph.png "GraphPaperQuantity")
Average number of image sets sorted per person over payrate;   

![Paper accuracy graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/Financial%20Incentives%20Accuracy%20over%20Pay%20graph.png "GraphPaperAccuracy")
Proportion of correctly sorted sets over payrate.

![Paper value graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/Financial%20Incentives%20Perceived%20Task%20Value.png "GraphPaperVal")
Graphs taken from “Financial Incentives and the ‘Performance of Crowds’”
Perceived value of tasks done by the workers.

The second experiment consisted of asking workers to find words in a crossword puzzle. Workers on Mechanical Turk were given a word search puzzle and list of words that may be found in the puzzle, not all of which were in the puzzle. Thus participants did not know what or how many words from the list were in the puzzle. There were 2 models tested in the experiment, a quota model, where workers were paid by puzzle and a piece-rate model, where workers were paid per word found. Each model had varying payment tiers. For the per-puzzle model, workers were either paid $0.01, $0.05, or $0.10 per puzzle, and for the per-word category, workers were paid $0.01, $0.02, or $0.03.

The task was posted on Mechanical Turk with the title “Solve Word Jumble Puzzles”. When the task was accepted, participants were asked to answer a questionnaire and were given a test puzzle. The participants were informed in bold text that not all words could be found in the puzzle. After finishing the test puzzle, participants received feedback on their performance and were told how they would be paid (per puzzle or per word) for bonus puzzles and how much they would be paid. They would be able to complete up to 24 additional puzzles, with infinite time per puzzle, the ability to go onto the next puzzle or finish the hit whenever they wanted. When they chose to finish the task, they were asked a questionnaire, given feedback on puzzles, and paid appropriately.

320 workers participated in this experiment. The demographics for age and location were very similar to the first experiment. Gender demographics differed from the first experiment, where 74.1% reported female. The income levels differed a bit as well; with a greater shift towards the $30-70k income bracket. 

Results showed that participants who were paid for bonus puzzles completed more than those who weren’t, which is consistent with the first experiment. However, there were no differences in quantity of work between the two groups, despite the pay-per-word scheme paying out more than the per puzzle scheme. The researchers attributed it to intrinsic motivation, as workers surveyed after the task indicated a strong positive relationship between enjoyment of the task and the number of puzzles completed. Those who said they enjoyed the task the most completed 3 times as many puzzles as those who only enjoyed the task a little. Paralleling the findings of the last experiment, the accuracy of the work did not increase as price increased for both methods of compensation. Accuracy was measured by the amount of words found in a puzzle over the amount of words that were in the puzzle. Participants in the per-word category overall earned over four times the amount that the per puzzle participants did, but despite the greater earnings, participants in the per word group found the least amount of words among the groups. Surprisingly, participants who did not receive any pay beyond the test puzzle found the on average 85.6% of the words per puzzle. Those who were paid per puzzle found on average 84.7% of the words, and those paid per word found on average 81.4% of the words. 

Researchers explained that the per-puzzle model of payment elicits work by encouraging greater marginal effort for harder-to-find words and sets an implicit goal for the participant to complete the entire puzzle. Researchers also found that similar to the first experiment, each of the task groups felt like they should have been paid more than they were. In this experiment, those who were not paid past the test puzzle also were surveyed on how much they thought they should have been paid. Surprisingly, their perceived pay was greater than the rating that the lowest paid group. The researchers explain this through the expectations of the participant. With no financial expectation, effort is motivated by other rewards, such as enjoyment, however with financial expectation, money is the main motivator. 

In summary, through these two experiments researchers found that increased payments increased the quantity of work, but did not have much effect on the quality of work obtained. Secondly, the model by which a worker is paid (quota vs per unit) has an effect on quality of the work done and that using the quota model may be cheaper and provide better quality than that of the per unit model. 

![Paper value graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/Financial%20Incentives%20Accuracy%20WordSearch.png "GraphPaper")

![value Wsearch graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/Financial%20Incentives%20Perceived%20WordSearch.png
 "WSearchGraph")
Graphs taken from “Financial Incentives and the ‘Performance of Crowds’”

#### Why We Chose to Replicate This Experiment

Our group decided to implement the experiment involving differing pay grades and difficulties per task. We decided that this experiment would be interesting to perform in order to get a scope of how much workers will do given variation of the above variables. This information would give us a better idea of how we should structure the data collection of our final project in order to fit the ability and willingness of the crowd. We chose this experiment over the other in the paper because we may be using the pay per small task model in our final project. This experiment also seemed fairly straightforward to implement and can be completed by crowdworkers fairly quickly, so it could fit our time schedule that we had. 

#### Experimental Setup

In the original experiment, the researchers had workers arrange a series of images in chronological order. However, since we did not have access to a similar dataset of images sorted by chronological order, we decided to have workers unscramble sentences instead. This is analogous to the design of the original experiment, because workers are still arranging items in the correct order; the medium has just been changed from images to sentences. The number of items in each sentence is greater than the number of images originally used, but this is compensated for by the reduced cognitive load of unscrambling a sentence — the words can be read and understood at a glance, whereas the traffic images require closer examination. Each worker had the option to unscramble up to 30 sentences, each of the same (randomly selected) difficulty level, for a constant per-sentence bonus that was also randomly selected.

We also conducted our experiment on a smaller scale than the one in the paper — we used smaller sets of items per HIT (a maximum of 30 sentences, as opposed to 99 sets of images), and limited the number of workers to 81 total. We made sure to maintain an even distribution of difficulty and pay grades across the workers, however, thus mimicking the original experiment’s invariants. The scale of the payments for our experiment was also smaller: the researchers paid their workers a base rate of $0.10 per HIT regardless of number of bonus tasks actually done, and then a rate of either $0.01, $0.05, or $0.10 per image set completed. However, due to budget constraints, we chose to pay workers a base rate of $0.05 per HIT, and then a randomly chosen rate of $0.01, $0.03, or $0.05 per sentence completed. (This aligned with the researchers’ practice of using their max payout as a base rate.) Difficulties were decided using sentence fragments parsed from a corpus of sentences organized in dependency trees from training data from a Stanford dependency parser, located here: <http://nlp.stanford.edu/software/parser-faq.shtml#z>. We created our own parser that split sentences into fragments based on a parse depth value. The depth determined the number of levels of the dependency parse to search before returning all the text below as a unit. Easy had a parse depth of 3, medium a parse depth of 5, and hard a parse depth of 10. Finally, we omitted the demographic and perceived value surveys from our experiment, because although they added interesting information in the original paper, we determined that the extra complexity was not necessary for us to replicate the core of the experiment.

> Depth 3: Drive to the next set of traffic lights ||| and ||| then ||| turn left ||| .
> Depth 5: Drive ||| to ||| the next set of traffic lights ||| and ||| then ||| turn ||| left ||| .
> Depth 10: Drive ||| to ||| the ||| next ||| set ||| of ||| traffic ||| lights ||| and ||| then ||| turn ||| left ||| .

Our measure of accuracy was also different than the measures used in the experiment. Since some of the sentences obtained from the training data had somewhat ambiguous arrangements or were fairly difficult to reorder correctly due to the original sentence structure, we decided it was more appropriate to calculate the accuracy through the number of inversions between the given answer and the correct answer. We wrote a Python script to do this. Since our main goal in reproducing the paper is to see the variation of accuracy between price brackets and not the accuracy scores themselves, we felt that this change in measurement was appropriate.

To see the original HIT instructions, as well as a screenshot of how it appeared to workers, see the HIT_instructions.md file. The Github repo for the code we wrote (including Python scripts and the Javascript for the HIT itself) is located at <github.com/john-hewitt/scrambled>.

#### Crowdsourcing Setup

In order to view and accept one of our HITs, we required workers to be located in the US and have a HIT acceptance rate of no less than 80%. When reviewing HIT submissions, we rejected workers who submitted multiple times and whose responses were completely blank. Workers who had at least one non-empty submission had that submission accepted, but any extra or blank submissions rejected were rejected. For each HIT that we had to reject a submission for, we added one more assignment so that another worker could complete it. We repeated this process until we had no duplicate submissions. After the HITs were completed, we counted up the amount of sentences completed per worker and awarded each worker a bonus of (cost per sentence) multiplied by (number of sentences completed).

#### Analysis of Results

Our experiment split workers into 9 categories: the cross product of (easy, medium, hard) difficulties and (0.01, 0.03 0.05) cents paid per sentence. Recall that (Mason and Watts, 2009) found a direct relationship between payment and quantity of work done by workers on Mechanical Turk. They found no relationship between payment and the quality of work done. Our experiment replicated the second result, finding no correlation between payment and quality of work done, but was unable to replicate the first — we found no correlation between payment and quantity of work done. Further, we found little correlation between difficulty of task and quality of work done. While the ‘easy’ sentences scored the highest, we were surprised to find that the average descrambling quality of hard sentences to be higher than the average descrambling quality of medium sentences.

![Quantity graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/resultsQuantity.png "QuantGraph")
Standard deviations are too large to be meaningful when displayed as error bars

![Accuracy graph](https://raw.githubusercontent.com/john-hewitt/scrambled/master/resultsAccuracy.png
 "AccGraph")
Note the high accuracy of the sets, good job turkers!

There are a number of reasons why we did not find a relationship between payment and quantity of work done; one of them is most likely our small sample size. In contrast to the 611 workers who participated in the original experiment, we only had 81, with 9 workers per category of (difficulty) x (pay rate). This meant that any outlier workers had a disproportionately large effect on the results. Also, although we did not see an overall trend in the relationship between payment and number of tasks completed, there were a few data points of note: at the ‘easy’ difficulty level, 10 workers completed all 30 available sentences, evenly distributed across pay rates; at the ‘medium’ difficulty level, 5 workers completed all 30 sentences, also roughly evenly distributed across pay rates. In comparison, at the ‘hard’ difficulty level, no one did all 30 sentences, but 3 workers did more than 20 sentences, again evenly distributed across the three pay rates. 

These findings go directly against the findings of the original paper, but we think they make sense in the context of our task: the easy sentences are almost trivial to put in order for an English-speaker (for example, “a stick of chewing gum ||| I ||| . ||| ate” is clearly “I ||| ate ||| a stick of chewing gum ||| .”), making it easy to quickly complete 30 of them, but as the difficulty level increases, the time per sentence increases — most likely to the point where workers considered it not worth it. For example, one of the difficult sentences was “frees ||| Humans ||| limbs ||| for ||| an ||| that ||| body ||| upper ||| using tools ||| and ||| erect ||| their ||| carriage ||| have ||| manipulating objects ||| . |||”. Unscrambled, it reads “Humans ||| have ||| an ||| erect ||| body ||| carriage ||| that ||| frees ||| their ||| upper ||| limbs ||| for ||| using tools ||| and ||| manipulating objects ||| . |||” — a daunting sentence to assemble word-by-word. The large standard deviation found in average sentence counts was thus likely due to individual variation between workers’ patience and English literacy level.

This would also explain our surprising result that the average accuracy for hard sentences was higher than the average accuracy for medium sentences — workers who completed a large number of hard sentences probably did so because they wanted to take on the challenge, and were both patient and skilled enough to do it correctly. This last factor may also have contributed to the lack of a positive correlation between pay and number of tasks completed — if workers assumed that they needed to get each sentence correct in order to be paid for it, this may have discouraged them from taking the time to complete more, even at higher pay rates. It was unclear from the original paper what the exact wording used by the researchers was when describing how they would pay for each task; we made the conscious decision to not describe how accuracy would affect payment. (Telling workers they would only get paid for each correct sentence would have been a discouraging lie; telling workers they would be paid regardless of accuracy would probably have resulted in a large number of junk inputs.) Finally, we scaled down our pay rates in order to stay within our college-student budgets — where the researchers paid workers a maximum of $0.10 per sentence, we maxed out at $0.05 per sentence, which may have provided a weaker incentive to do more sentences at a higher pay rate. This would also have contributed to our lack of a relationship between pay and quantity.

Overall, our small sample size and relative difficulty level of the task we chose probably caused our results to deviate from those of the original experiment. We did our best to choose a task that would be analogous to the original, but time and budget constraints meant that our experimental conditions were not directly parallel to those in the paper. Even though we did not find a significant relationship between pay and quantity of work done, our results did confirm that there appears to be no relationship between pay and quality of work done — higher pay does not mean higher quality. Given the chance to do it again (with more time and funding), we would increase the sample size and pay rates to match those of the original paper and see if we could replicate the results more closely.
